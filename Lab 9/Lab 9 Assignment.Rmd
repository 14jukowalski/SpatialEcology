---
title: "R Notebook"
output: html_notebook
---

```{r, warning=F, message=F}

rm(list=ls())

require(sf)
require(tidyterra)
require(dismo)
require(tidyverse)
require(terra)
require(predicts)
require(ggnewscale)
require(mgcv)
require(randomForest)
require(maxnet)
require(enmSdmX)
require(gbm)
require(PresenceAbsence)
require(ecospat)
```

# This first code chunk just recreates the maps we built in the lab.

```{r}

# Model building data
vathData = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_2004.csv')

vathPres = vathData %>% filter(VATH==1)
vathAbs = vathData %>% filter(VATH==0)

vathPresXy = as.matrix(vathPres %>% select(EASTING, NORTHING))
vathAbsXy = as.matrix(vathAbs %>% select(EASTING, NORTHING))



# Validation data
vathVal = read.csv('https://raw.githubusercontent.com/ValenteJJ/SpatialEcology/main/Week8/vath_VALIDATION.csv')

vathValPres = vathVal %>% filter(VATH==1)
vathValAbs = vathVal %>% filter(VATH==0)

vathValXy = as.matrix(vathVal %>% select(EASTING, NORTHING))
vathValPresXy = as.matrix(vathValPres %>% select(EASTING, NORTHING))
vathValAbsXy = as.matrix(vathValAbs %>% select(EASTING, NORTHING))



# Bringing in the covariates
elev = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/elevation.tif')
canopy = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/canopy.tif')
mesic = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/mesic.tif')
precip = rast('https://github.com/ValenteJJ/SpatialEcology/raw/main/Week8/precip.tif')


# Resampling to make the covariate rasters match
mesic = resample(x = mesic, y = elev, 'near')
precip = resample(x = precip, y = elev, 'bilinear')

mesic = mask(mesic, elev)
precip = mask(precip, elev)

# Mesic forest within 1 km
probMatrix = focalMat(mesic, 1000, type='circle', fillNA=FALSE)
mesic1km = focal(mesic, probMatrix, fun='sum')


# Building the raster stack
layers = c(canopy, elev, mesic1km, precip)
names(layers) = c('canopy', 'elev', 'mesic1km', 'precip')


#Creating background points
set.seed(23)

backXy = data.frame(backgroundSample(layers, n=2000, p=vathPresXy))

# Extracting covariates for our different points
presCovs = extract(layers, vathPresXy)
absCovs = extract(layers, vathAbsXy)
backCovs = extract(layers, backXy)
valCovs = extract(layers, vathValXy)

presCovs = data.frame(vathPresXy, presCovs, pres=1)
absCovs = data.frame(vathAbsXy, absCovs, pres=0)
backCovs = data.frame(backXy, backCovs, pres=0)
valCovs = data.frame(vathValXy, valCovs)

presCovs = presCovs[complete.cases(presCovs),]
absCovs = absCovs[complete.cases(absCovs),]
backCovs = backCovs[complete.cases(backCovs),]

# Combining presence and background data into one dataframe

backCovs = backCovs %>% select(-ID)
colnames(presCovs)[1:2] = c('x', 'y')
colnames(absCovs)[1:2] = c('x', 'y')

presBackCovs = rbind(presCovs, backCovs)
presAbsCovs = rbind(presCovs, absCovs)


# Fitting bioclim envelope model
tmp = presCovs %>% select(elev, precip, mesic1km, canopy) %>% 
  as.matrix()

bioclim = envelope(tmp)

bioclimMap = predict(layers, bioclim)



# Fitting GLM
glmModel = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmMap = predict(layers, glmModel, type='response')


# Fitting GAM
gamModel = gam(pres ~ s(canopy, k=6) + s(elev, k=6) + s(mesic1km, k=6) + s(precip, k=6), family='binomial', data=presBackCovs, method='ML')

gamMap = predict(layers, gamModel, type='response')


# Fitting boosted regression tree model

boostModel = gbm(pres ~ elev + canopy + mesic1km + precip, distribution='bernoulli', n.trees=100, interaction.depth=2, shrinkage=0.1, bag.fraction=0.5, data=presBackCovs)

boostMap = predict(layers, boostModel, type='response')
boostMap = mask(boostMap, layers$canopy)


# Fitting random forest model

rfModel = randomForest(as.factor(pres) ~ canopy + elev + mesic1km + precip, data=presBackCovs, mtry=2, ntree=500, na.action = na.omit)

rfMap = predict(layers, rfModel, type='prob', index=2)


#Fitting maxent model

pbVect = presBackCovs$pres
covs = presBackCovs %>% select(canopy:precip)

maxentModel = maxnet(p = pbVect,
                     data= covs,
                     regmult = 1,
                     classes='lqpht')


maxentMap = predictMaxNet(maxentModel, layers, type='logistic')
```


# Challenge 1 (4 points)

In the lab, we fit 6 SDMs. We then calculated discrimination statistics for all 6 and a calibration plot for 1 of them. Create calibration plots for the remaining 5 models, and then make a decision (based on your suite of discrimination statistics and calibration plots) about which of your SDMs is "best." Defend your answer.

```{r}
# Preparing data for calibration plots
tmp = valCovs %>% mutate(VATH = vathVal$VATH)
tmp = tmp[complete.cases(tmp),]

valData = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         bioVal = predict(bioclim, tmp %>% select(canopy:precip)),
         glmVal = predict(glmModel, tmp %>% select(canopy:precip), type='response'),
         gamVal = predict(gamModel, tmp %>% select(canopy:precip), type='response'),
         boostVal = predict(boostModel, tmp %>% select(canopy:precip), type='response'),
         rfVal = predict(rfModel, tmp %>% select(canopy:precip), type='prob')[,2],
         maxentVal = predict(maxentModel, tmp %>% select(canopy:precip), type='logistic')[,1])

summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData[,2], valData[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData[,i+2]*valData[,2] + (1-valData[,i+2]) * (1-valData[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData[,i+2] + 0.01)*valData[,2] + log((1-valData[,i+2]))*(1-valData[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval = rbind(summaryEval, summaryI)
}

summaryEval = summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData)[3:8])

summaryEval

par(mfcol=c(2,3))
glmcal = calibration.plot(valData, which.model=2, N.bins=20, xlab='predicted', ylab='Observed', main='glm')

bioclimcal = calibration.plot(valData, which.model=1, N.bins=20, xlab='predicted', ylab='Observed', main='bioclim')

gamcal = calibration.plot(valData, which.model=3, N.bins=20, xlab='predicted', ylab='Observed', main='gam')

boostcal = calibration.plot(valData, which.model=4, N.bins=20, xlab='predicted', ylab='Observed', main='boosted')

rfcal = calibration.plot(valData, which.model=5, N.bins=20, xlab='predicted', ylab='Observed', main='random forest')

maxentcal = calibration.plot(valData, which.model=6, N.bins=20, xlab='predicted', ylab='Observed', main='maxent')



```

Based on the discrimination statistics and calibration plots I believe that the glm species distribution model is the best model of those considered. I think the glm is the best model compared to the other models because it had the highest value for the most of the discrimination statistics including AUC, correlation, and true skill statistic. The glm also had values near the top for other statistics as well including kappa, and specificity. Additionally, the glm species distribution model calibration plot was close to the 1:1 line except for high values and was closer to the 1:1 line for all other models except for the gam. But, even though I think the glm is the best model of those considered, I do not think it is a good model.  



# Challenge 2 (4 points)

Each SDM we created uses a different algorithm with different assumptions. Because of this, ecologists frequently use "ensemble" approaches that aggregate predictions from multiple models in some way. Here we are going to create an ensemble model by calculating a weighted average of the predicted occupancy values at each pixel. We will calculate weights based on model AUC values to ensure that the models with the best AUC values have the most influence on the predicted values in the ensemble model.

Create a raster stack that combines the glmMap, gamMap, boostMap, and rfMap (hint use c()).

Next, create a vector of the AUC values for each model.

Lastly, use the weighted.mean() function in the terra package to create the new raster as a weighted average of the previous 4 rasters.

Plot the result, and explain why we left out the bioclim and Maxent models for this ensemble model.

```{r}
#raster stack
rasterstack = stack(c(glmMap, gamMap, boostMap, rfMap))
AUCvector = c(0.6726221, 0.6455923, 0.6403391, 0.6322577)

waraster = weighted.mean(rasterstack,AUCvector)
plot(waraster)



```

The reason we left out the bioclim and maxent models is because those models are based off of only presence data where as the four models we included in our ensemble model use presence and absence data. Therefore, the models can not be compared because they are not built using the same data and have different baselines that are used to anchor the model. 


# Challenge 3 (4 points)

Is this ensemble model an improvement over one of the models you built previously? Provide evidence and explain the criteria you used to come to your conclusion.

```{r}
presensval = extract(waraster, vathPresXy)
backensval = extract(waraster, backXy)
valensval = extract(waraster, vathValXy)

presensval = data.frame(vathPresXy, presensval, pres=1)
backensval = data.frame(backXy, backensval, pres=0)
valensval = data.frame(vathValXy, valensval)

presensval = presensval[complete.cases(presensval),]
backensval = backensval[complete.cases(backensval),]

colnames(presensval)[1:3] = c('x', 'y', 'modval')
colnames(backensval)[1:3] = c('x', 'y', 'modval')

# ensvaldata = rbind(presensval, backensval)

tmp1 = valensval %>% mutate(VATH = vathVal$VATH)
tmp1 = tmp1[complete.cases(tmp1),]

valData = data.frame('ID' = 1:nrow(tmp1)) %>% 
  mutate(obs = tmp1$VATH,
         modval = tmp1$valensval)

summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData[,2], valData[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData[,i+2]*valData[,2] + (1-valData[,i+2]) * (1-valData[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData[,i+2] + 0.01)*valData[,2] + log((1-valData[,i+2]))*(1-valData[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval = rbind(summaryEval, summaryI)
}

summaryEval = summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData)[3])

summaryEval

```

I believe that the ensemble model is a better model for all of the models used to make the ensamble model except for the glm model. I believe this because the values of auc, tss, and kappa are higher for the ensamble model than for the gam, boosted regression tree, and random forest models. I used these criteria because auc, tss, and kappa are used to evaluate models based on validation data and since those values are higher for the ensemble model I believe the ensemble model is better then the gam, boosted regression tree, and random forest models.



# Challenge 4 (4 points)

In the lab we built models using presence-background data then validated those models with presence-absence data. For this challenge, you're going to compare the predictive ability of a model built using presence-background data with one built using presence-absence data.

Fit a GLM using the presence-background data as we did in the lab (i.e., use the presBackCovs dataframe). Fit a second GLM using the presence-absence data (i.e., use the presAbsCovs dataframe). Validate both of these models on the novel presence-absence data (valCovs dataset). Specifically, calculate and compare AUC, Kappa, and TSS for these two models. Which model does a better job of prediction for the validation data and why do you think that is? 

```{r}
glmModelback = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presBackCovs)

glmmodelabs = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=presAbsCovs)


valData1 = data.frame('ID' = 1:nrow(tmp)) %>% 
  mutate(obs = tmp$VATH,
         glmbackVal = predict(glmModelback, tmp %>% select(canopy:precip), type='response'),
         glmabsVal = predict(glmmodelabs, tmp %>% select(canopy:precip), type='response'))
         
summaryEval = data.frame(matrix(nrow=0, ncol=9))

nModels = ncol(valData1)-2


for(i in 1:nModels){
  
  #AUC
  auc = auc(valData1, which.model = i)
  
  #Find threshold to maximize Kappa
  kappaOpt = optimal.thresholds(valData1, which.model = i, opt.methods=3)
  
  #Sensitivity
  sens = sensitivity(cmx(valData1, which.model=i, threshold = kappaOpt[[2]]))
  
  #Specificity
  spec = specificity(cmx(valData1, which.model = i, threshold = kappaOpt[[2]]))
  
  #True skill statistic
  tss = sens$sensitivity + spec$specificity - 1
  
  #Kappa
  kappa = Kappa(cmx(valData1, which.model = i, threshold = kappaOpt[[2]]))
  
  #Correlation between predicted and realized values
  corr = cor.test(valData1[,2], valData1[,i+2])$estimate
  
  #Log likelihood
  ll = sum(log(valData1[,i+2]*valData1[,2] + (1-valData1[,i+2]) * (1-valData1[,2])))
  ll = ifelse(ll == '-Inf', sum(log(valData1[,i+2] + 0.01)*valData1[,2] + log((1-valData1[,i+2]))*(1-valData1[,2])), ll)
  
  #Put them all together and save the values
  summaryI = c(i, auc$AUC, corr, ll, kappaOpt[[2]], sens$sensitivity, spec$specificity, tss, kappa[[1]])
  summaryEval = rbind(summaryEval, summaryI)
}

summaryEval = summaryEval %>% 
  setNames(c('model', 'auc', 'corr', 'll', 'threshold', 'sens', 'spec', 'tss', 'kappa')) %>% 
  mutate(model = colnames(valData1)[3:4])

summaryEval
```

The glm with the presence-absence data appears to be a better model than the glm created using presence-background data. I believe this is the case as the glm with the absence data had higher auc and TSS, although the model with the background data had a better kappa value. I think the model with the absence data is the better model because the absence data represents locations where the species was not found where as the background data are random points and may be located in areas where the species may actually exist. Therefore, the model with the absence points likely does a better job of differentiating where species are located and where they are not and does a better job of associating covariate values with where species are and are not located.  



# Challenge 5 (4 points)

Now calculate the same statistics (AUC, Kappa, and TSS) for each model you developed in Challenge 4 using K-fold validation with 5 groups. Do these models perform better or worse based on K-fold validation (as compared to validation based on novel data)? Why might that occur?

```{r}
set.seed(23)

nFolds = 5
kfoldPres = kfold(presCovs, k=nFolds)
kfoldBack = kfold(backCovs, k=nFolds)
kfoldAbs = kfold(absCovs, k=nFolds)
valback = rbind(presCovs,backCovs)
aucVals = rep(NA, nFolds)

i = 1

 valPres = valback[kfoldAbs==i,]
  
  trainPres = presCovs[kfoldPres!=i,]
  trainBack = backCovs[kfoldBack!=i,]
  trainBoth = rbind(trainPres, trainBack)
  
glmModelback = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBoth)



valData1 = data.frame('ID' = 1:nrow(valPres)) %>% 
  mutate(obs = valPres$pres,
         glmbackVal = predict(glmModelback, valPres %>% select(canopy:precip), type='response'),
  glmabsVal = predict(glmModelabs, valPres %>% select(canopy:precip), type='response'))
         
tmp = auc(valData1, which.model=i)  


aucVals[i] = auc(valData1, which.model = i)[1]



#background points
for(i in 1:nFolds){
  valPres1 = presCovs[kfoldPres==i,]
  
  trainPres = presCovs[kfoldPres!=i,]
  trainBack = backCovs[kfoldBack!=i,]
  trainAbs = absCovs[kfoldAbs!=i,]
  trainBoth = rbind(trainPres, trainBack)
  trainBothabs = rbind(trainPres, trainAbs)
  
glmModelback = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBoth)
glmModelabs = glm(pres ~ canopy + elev + I(elev^2) + mesic1km + precip, family='binomial', data=trainBothabs)


valData1 = data.frame('ID' = 1:nrow(valPres)) %>% 
  mutate(obs = valPres$pres,
         glmbackVal = predict(glmModelback, valPres %>% select(canopy:precip), type='response'),
  glmabsVal = predict(glmModelabs, valPres %>% select(canopy:precip), type='response'))
         
  
aucVals[i] = auc(valData1, which.model = i)

}

```

I give up... R wins....

But, the k-fold data set should perform better than the novel validation data because the AUC, TSS, and kappa should be higher for a data set that is validated from data that came from that data set. This is because validation data that came from the original dataset were collected under the same conditions and in the same location as the data that is being validated.  
